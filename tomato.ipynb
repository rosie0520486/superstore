{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODUmDR7q3oSYjTSOMwx1Be",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rosie0520486/superstore/blob/main/tomato.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GSWD2W1htD5",
        "outputId": "216ce911-3995-4ce9-c0ad-84d01ec3d4bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.11/dist-packages (2.6.0)\n",
            "Requirement already satisfied: gymnasium<1.2.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (1.1.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.0.2)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.6.0+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3) (4.14.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2025.7.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.2)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n",
            "Collecting shimmy>=2.0\n",
            "  Downloading Shimmy-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from shimmy>=2.0) (2.0.2)\n",
            "Requirement already satisfied: gymnasium>=1.0.0a1 in /usr/local/lib/python3.11/dist-packages (from shimmy>=2.0) (1.1.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (4.14.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (0.0.4)\n",
            "Downloading Shimmy-2.0.0-py3-none-any.whl (30 kB)\n",
            "Installing collected packages: shimmy\n",
            "Successfully installed shimmy-2.0.0\n",
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 581  |\n",
            "|    iterations      | 1    |\n",
            "|    time_elapsed    | 3    |\n",
            "|    total_timesteps | 2048 |\n",
            "-----------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 507           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 8             |\n",
            "|    total_timesteps      | 4096          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.4610275e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.77         |\n",
            "|    explained_variance   | -1.67e-06     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.86e+10      |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -1.05e-05     |\n",
            "|    value_loss           | 8.91e+10      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 482           |\n",
            "|    iterations           | 3             |\n",
            "|    time_elapsed         | 12            |\n",
            "|    total_timesteps      | 6144          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.1979137e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.77         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 3.02e+11      |\n",
            "|    n_updates            | 20            |\n",
            "|    policy_gradient_loss | -9.61e-06     |\n",
            "|    value_loss           | 6.21e+11      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 457          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 17           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.043127e-09 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.77        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.38e+11     |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -2.95e-06    |\n",
            "|    value_loss           | 1.71e+12     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 445           |\n",
            "|    iterations           | 5             |\n",
            "|    time_elapsed         | 22            |\n",
            "|    total_timesteps      | 10240         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.5716068e-09 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -2.77         |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.72e+12      |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | -2.09e-06     |\n",
            "|    value_loss           | 3.35e+12      |\n",
            "-------------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 435       |\n",
            "|    iterations           | 6         |\n",
            "|    time_elapsed         | 28        |\n",
            "|    total_timesteps      | 12288     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 2.73e+12  |\n",
            "|    n_updates            | 50        |\n",
            "|    policy_gradient_loss | -2.14e-06 |\n",
            "|    value_loss           | 5.46e+12  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 439       |\n",
            "|    iterations           | 7         |\n",
            "|    time_elapsed         | 32        |\n",
            "|    total_timesteps      | 14336     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 4.03e+12  |\n",
            "|    n_updates            | 60        |\n",
            "|    policy_gradient_loss | -1.15e-06 |\n",
            "|    value_loss           | 8.1e+12   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 442       |\n",
            "|    iterations           | 8         |\n",
            "|    time_elapsed         | 36        |\n",
            "|    total_timesteps      | 16384     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 5.69e+12  |\n",
            "|    n_updates            | 70        |\n",
            "|    policy_gradient_loss | -1.11e-06 |\n",
            "|    value_loss           | 1.13e+13  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 440       |\n",
            "|    iterations           | 9         |\n",
            "|    time_elapsed         | 41        |\n",
            "|    total_timesteps      | 18432     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 7.35e+12  |\n",
            "|    n_updates            | 80        |\n",
            "|    policy_gradient_loss | -1.53e-06 |\n",
            "|    value_loss           | 1.5e+13   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 444       |\n",
            "|    iterations           | 10        |\n",
            "|    time_elapsed         | 46        |\n",
            "|    total_timesteps      | 20480     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 9.5e+12   |\n",
            "|    n_updates            | 90        |\n",
            "|    policy_gradient_loss | -2.17e-07 |\n",
            "|    value_loss           | 1.92e+13  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 447       |\n",
            "|    iterations           | 11        |\n",
            "|    time_elapsed         | 50        |\n",
            "|    total_timesteps      | 22528     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 1.21e+13  |\n",
            "|    n_updates            | 100       |\n",
            "|    policy_gradient_loss | -2.14e-06 |\n",
            "|    value_loss           | 2.4e+13   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 443       |\n",
            "|    iterations           | 12        |\n",
            "|    time_elapsed         | 55        |\n",
            "|    total_timesteps      | 24576     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 1.48e+13  |\n",
            "|    n_updates            | 110       |\n",
            "|    policy_gradient_loss | -3.43e-07 |\n",
            "|    value_loss           | 2.92e+13  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 445       |\n",
            "|    iterations           | 13        |\n",
            "|    time_elapsed         | 59        |\n",
            "|    total_timesteps      | 26624     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 1.76e+13  |\n",
            "|    n_updates            | 120       |\n",
            "|    policy_gradient_loss | -1.96e-07 |\n",
            "|    value_loss           | 3.49e+13  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 444       |\n",
            "|    iterations           | 14        |\n",
            "|    time_elapsed         | 64        |\n",
            "|    total_timesteps      | 28672     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 2.11e+13  |\n",
            "|    n_updates            | 130       |\n",
            "|    policy_gradient_loss | -8.39e-08 |\n",
            "|    value_loss           | 4.13e+13  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 445       |\n",
            "|    iterations           | 15        |\n",
            "|    time_elapsed         | 68        |\n",
            "|    total_timesteps      | 30720     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 2.4e+13   |\n",
            "|    n_updates            | 140       |\n",
            "|    policy_gradient_loss | -1.55e-06 |\n",
            "|    value_loss           | 4.83e+13  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 447       |\n",
            "|    iterations           | 16        |\n",
            "|    time_elapsed         | 73        |\n",
            "|    total_timesteps      | 32768     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 2.73e+13  |\n",
            "|    n_updates            | 150       |\n",
            "|    policy_gradient_loss | -5.64e-07 |\n",
            "|    value_loss           | 5.58e+13  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 445       |\n",
            "|    iterations           | 17        |\n",
            "|    time_elapsed         | 78        |\n",
            "|    total_timesteps      | 34816     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 3.16e+13  |\n",
            "|    n_updates            | 160       |\n",
            "|    policy_gradient_loss | -9.64e-08 |\n",
            "|    value_loss           | 6.34e+13  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 445       |\n",
            "|    iterations           | 18        |\n",
            "|    time_elapsed         | 82        |\n",
            "|    total_timesteps      | 36864     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | -3.58e-07 |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 3.6e+13   |\n",
            "|    n_updates            | 170       |\n",
            "|    policy_gradient_loss | -1.09e-06 |\n",
            "|    value_loss           | 7.17e+13  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 446       |\n",
            "|    iterations           | 19        |\n",
            "|    time_elapsed         | 87        |\n",
            "|    total_timesteps      | 38912     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | 2.38e-07  |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 4.02e+13  |\n",
            "|    n_updates            | 180       |\n",
            "|    policy_gradient_loss | -5.16e-07 |\n",
            "|    value_loss           | 8.08e+13  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 443       |\n",
            "|    iterations           | 20        |\n",
            "|    time_elapsed         | 92        |\n",
            "|    total_timesteps      | 40960     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 4.51e+13  |\n",
            "|    n_updates            | 190       |\n",
            "|    policy_gradient_loss | -6.92e-07 |\n",
            "|    value_loss           | 9.03e+13  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 444       |\n",
            "|    iterations           | 21        |\n",
            "|    time_elapsed         | 96        |\n",
            "|    total_timesteps      | 43008     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 4.98e+13  |\n",
            "|    n_updates            | 200       |\n",
            "|    policy_gradient_loss | -7.04e-07 |\n",
            "|    value_loss           | 1e+14     |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 443       |\n",
            "|    iterations           | 22        |\n",
            "|    time_elapsed         | 101       |\n",
            "|    total_timesteps      | 45056     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 5.47e+13  |\n",
            "|    n_updates            | 210       |\n",
            "|    policy_gradient_loss | -5.55e-07 |\n",
            "|    value_loss           | 1.1e+14   |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 443       |\n",
            "|    iterations           | 23        |\n",
            "|    time_elapsed         | 106       |\n",
            "|    total_timesteps      | 47104     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 5.95e+13  |\n",
            "|    n_updates            | 220       |\n",
            "|    policy_gradient_loss | -7.77e-07 |\n",
            "|    value_loss           | 1.21e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 445       |\n",
            "|    iterations           | 24        |\n",
            "|    time_elapsed         | 110       |\n",
            "|    total_timesteps      | 49152     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | 5.96e-08  |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 6.65e+13  |\n",
            "|    n_updates            | 230       |\n",
            "|    policy_gradient_loss | -1.61e-07 |\n",
            "|    value_loss           | 1.33e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 443       |\n",
            "|    iterations           | 25        |\n",
            "|    time_elapsed         | 115       |\n",
            "|    total_timesteps      | 51200     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 7.24e+13  |\n",
            "|    n_updates            | 240       |\n",
            "|    policy_gradient_loss | -7.53e-07 |\n",
            "|    value_loss           | 1.45e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 443       |\n",
            "|    iterations           | 26        |\n",
            "|    time_elapsed         | 120       |\n",
            "|    total_timesteps      | 53248     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 7.89e+13  |\n",
            "|    n_updates            | 250       |\n",
            "|    policy_gradient_loss | -8.34e-07 |\n",
            "|    value_loss           | 1.57e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 442       |\n",
            "|    iterations           | 27        |\n",
            "|    time_elapsed         | 124       |\n",
            "|    total_timesteps      | 55296     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 8.63e+13  |\n",
            "|    n_updates            | 260       |\n",
            "|    policy_gradient_loss | -3.19e-08 |\n",
            "|    value_loss           | 1.71e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 440       |\n",
            "|    iterations           | 28        |\n",
            "|    time_elapsed         | 130       |\n",
            "|    total_timesteps      | 57344     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 8.99e+13  |\n",
            "|    n_updates            | 270       |\n",
            "|    policy_gradient_loss | -1.79e-06 |\n",
            "|    value_loss           | 1.84e+14  |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                   |          |\n",
            "|    fps                  | 441      |\n",
            "|    iterations           | 29       |\n",
            "|    time_elapsed         | 134      |\n",
            "|    total_timesteps      | 59392    |\n",
            "| train/                  |          |\n",
            "|    approx_kl            | 0.0      |\n",
            "|    clip_fraction        | 0        |\n",
            "|    clip_range           | 0.2      |\n",
            "|    entropy_loss         | -2.77    |\n",
            "|    explained_variance   | 0        |\n",
            "|    learning_rate        | 0.0003   |\n",
            "|    loss                 | 1e+14    |\n",
            "|    n_updates            | 280      |\n",
            "|    policy_gradient_loss | 3.65e-08 |\n",
            "|    value_loss           | 1.99e+14 |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 440       |\n",
            "|    iterations           | 30        |\n",
            "|    time_elapsed         | 139       |\n",
            "|    total_timesteps      | 61440     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 1.05e+14  |\n",
            "|    n_updates            | 290       |\n",
            "|    policy_gradient_loss | -6.07e-07 |\n",
            "|    value_loss           | 2.13e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 439       |\n",
            "|    iterations           | 31        |\n",
            "|    time_elapsed         | 144       |\n",
            "|    total_timesteps      | 63488     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 1.14e+14  |\n",
            "|    n_updates            | 300       |\n",
            "|    policy_gradient_loss | -6.91e-07 |\n",
            "|    value_loss           | 2.29e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 440       |\n",
            "|    iterations           | 32        |\n",
            "|    time_elapsed         | 148       |\n",
            "|    total_timesteps      | 65536     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | 5.96e-08  |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 1.24e+14  |\n",
            "|    n_updates            | 310       |\n",
            "|    policy_gradient_loss | -5.93e-07 |\n",
            "|    value_loss           | 2.45e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 438       |\n",
            "|    iterations           | 33        |\n",
            "|    time_elapsed         | 154       |\n",
            "|    total_timesteps      | 67584     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 1.31e+14  |\n",
            "|    n_updates            | 320       |\n",
            "|    policy_gradient_loss | -3.6e-07  |\n",
            "|    value_loss           | 2.61e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 439       |\n",
            "|    iterations           | 34        |\n",
            "|    time_elapsed         | 158       |\n",
            "|    total_timesteps      | 69632     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | 5.96e-08  |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 1.38e+14  |\n",
            "|    n_updates            | 330       |\n",
            "|    policy_gradient_loss | -5.31e-07 |\n",
            "|    value_loss           | 2.77e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 439       |\n",
            "|    iterations           | 35        |\n",
            "|    time_elapsed         | 163       |\n",
            "|    total_timesteps      | 71680     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 1.47e+14  |\n",
            "|    n_updates            | 340       |\n",
            "|    policy_gradient_loss | -3.01e-07 |\n",
            "|    value_loss           | 2.94e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 438       |\n",
            "|    iterations           | 36        |\n",
            "|    time_elapsed         | 168       |\n",
            "|    total_timesteps      | 73728     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | -2.38e-07 |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 1.55e+14  |\n",
            "|    n_updates            | 350       |\n",
            "|    policy_gradient_loss | -2.21e-07 |\n",
            "|    value_loss           | 3.12e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 438       |\n",
            "|    iterations           | 37        |\n",
            "|    time_elapsed         | 172       |\n",
            "|    total_timesteps      | 75776     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | -2.38e-07 |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 1.65e+14  |\n",
            "|    n_updates            | 360       |\n",
            "|    policy_gradient_loss | -4.36e-07 |\n",
            "|    value_loss           | 3.3e+14   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                   |          |\n",
            "|    fps                  | 436      |\n",
            "|    iterations           | 38       |\n",
            "|    time_elapsed         | 178      |\n",
            "|    total_timesteps      | 77824    |\n",
            "| train/                  |          |\n",
            "|    approx_kl            | 0.0      |\n",
            "|    clip_fraction        | 0        |\n",
            "|    clip_range           | 0.2      |\n",
            "|    entropy_loss         | -2.77    |\n",
            "|    explained_variance   | 1.79e-07 |\n",
            "|    learning_rate        | 0.0003   |\n",
            "|    loss                 | 1.73e+14 |\n",
            "|    n_updates            | 370      |\n",
            "|    policy_gradient_loss | -3.6e-07 |\n",
            "|    value_loss           | 3.5e+14  |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                   |          |\n",
            "|    fps                  | 437      |\n",
            "|    iterations           | 39       |\n",
            "|    time_elapsed         | 182      |\n",
            "|    total_timesteps      | 79872    |\n",
            "| train/                  |          |\n",
            "|    approx_kl            | 0.0      |\n",
            "|    clip_fraction        | 0        |\n",
            "|    clip_range           | 0.2      |\n",
            "|    entropy_loss         | -2.77    |\n",
            "|    explained_variance   | 2.98e-07 |\n",
            "|    learning_rate        | 0.0003   |\n",
            "|    loss                 | 1.82e+14 |\n",
            "|    n_updates            | 380      |\n",
            "|    policy_gradient_loss | 1.67e-07 |\n",
            "|    value_loss           | 3.7e+14  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 437       |\n",
            "|    iterations           | 40        |\n",
            "|    time_elapsed         | 187       |\n",
            "|    total_timesteps      | 81920     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 1.94e+14  |\n",
            "|    n_updates            | 390       |\n",
            "|    policy_gradient_loss | -6.15e-07 |\n",
            "|    value_loss           | 3.9e+14   |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                   |          |\n",
            "|    fps                  | 435      |\n",
            "|    iterations           | 41       |\n",
            "|    time_elapsed         | 192      |\n",
            "|    total_timesteps      | 83968    |\n",
            "| train/                  |          |\n",
            "|    approx_kl            | 0.0      |\n",
            "|    clip_fraction        | 0        |\n",
            "|    clip_range           | 0.2      |\n",
            "|    entropy_loss         | -2.77    |\n",
            "|    explained_variance   | 0        |\n",
            "|    learning_rate        | 0.0003   |\n",
            "|    loss                 | 2.04e+14 |\n",
            "|    n_updates            | 400      |\n",
            "|    policy_gradient_loss | 4.92e-08 |\n",
            "|    value_loss           | 4.11e+14 |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 435       |\n",
            "|    iterations           | 42        |\n",
            "|    time_elapsed         | 197       |\n",
            "|    total_timesteps      | 86016     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 2.14e+14  |\n",
            "|    n_updates            | 410       |\n",
            "|    policy_gradient_loss | -2.06e-07 |\n",
            "|    value_loss           | 4.32e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 434       |\n",
            "|    iterations           | 43        |\n",
            "|    time_elapsed         | 202       |\n",
            "|    total_timesteps      | 88064     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 2.25e+14  |\n",
            "|    n_updates            | 420       |\n",
            "|    policy_gradient_loss | -4.03e-07 |\n",
            "|    value_loss           | 4.53e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 434       |\n",
            "|    iterations           | 44        |\n",
            "|    time_elapsed         | 207       |\n",
            "|    total_timesteps      | 90112     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 2.34e+14  |\n",
            "|    n_updates            | 430       |\n",
            "|    policy_gradient_loss | 2.94e-07  |\n",
            "|    value_loss           | 4.75e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 434       |\n",
            "|    iterations           | 45        |\n",
            "|    time_elapsed         | 212       |\n",
            "|    total_timesteps      | 92160     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | -1.19e-07 |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 2.45e+14  |\n",
            "|    n_updates            | 440       |\n",
            "|    policy_gradient_loss | -1.92e-07 |\n",
            "|    value_loss           | 4.97e+14  |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                   |          |\n",
            "|    fps                  | 433      |\n",
            "|    iterations           | 46       |\n",
            "|    time_elapsed         | 217      |\n",
            "|    total_timesteps      | 94208    |\n",
            "| train/                  |          |\n",
            "|    approx_kl            | 0.0      |\n",
            "|    clip_fraction        | 0        |\n",
            "|    clip_range           | 0.2      |\n",
            "|    entropy_loss         | -2.77    |\n",
            "|    explained_variance   | 1.19e-07 |\n",
            "|    learning_rate        | 0.0003   |\n",
            "|    loss                 | 2.6e+14  |\n",
            "|    n_updates            | 450      |\n",
            "|    policy_gradient_loss | 9.31e-08 |\n",
            "|    value_loss           | 5.2e+14  |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 433       |\n",
            "|    iterations           | 47        |\n",
            "|    time_elapsed         | 222       |\n",
            "|    total_timesteps      | 96256     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | 1.19e-07  |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 2.72e+14  |\n",
            "|    n_updates            | 460       |\n",
            "|    policy_gradient_loss | -1.49e-07 |\n",
            "|    value_loss           | 5.43e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 432       |\n",
            "|    iterations           | 48        |\n",
            "|    time_elapsed         | 227       |\n",
            "|    total_timesteps      | 98304     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | 5.96e-08  |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 2.82e+14  |\n",
            "|    n_updates            | 470       |\n",
            "|    policy_gradient_loss | -1.19e-07 |\n",
            "|    value_loss           | 5.67e+14  |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 432       |\n",
            "|    iterations           | 49        |\n",
            "|    time_elapsed         | 231       |\n",
            "|    total_timesteps      | 100352    |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0       |\n",
            "|    clip_fraction        | 0         |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.77     |\n",
            "|    explained_variance   | 0         |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 2.97e+14  |\n",
            "|    n_updates            | 480       |\n",
            "|    policy_gradient_loss | -1.85e-07 |\n",
            "|    value_loss           | 5.92e+14  |\n",
            "---------------------------------------\n",
            "Step:1, Action:[3 1 0], State:[ 18.   10.  450.    1.5  30.    0. ], Reward:-30\n",
            "Step:2, Action:[1 1 0], State:[ 15.   10.  400.    1.5  40.    0. ], Reward:-40.0\n",
            "Step:3, Action:[2 0 1], State:[ 14.5  10.  500.    1.5  60.    0. ], Reward:-60.0\n",
            "Step:4, Action:[3 1 1], State:[ 15.25   10.    550.      1.625  90.      0.   ], Reward:-90.0\n",
            "Step:5, Action:[3 1 0], State:[ 15.625   10.     600.       1.9375 120.       0.    ], Reward:-120.0\n",
            "Step:6, Action:[3 1 0], State:[ 15.8125   10.      650.        2.34375 150.        0.     ], Reward:-150.0\n",
            "Step:7, Action:[2 1 0], State:[ 14.90625  10.      650.        2.34375 170.        0.     ], Reward:-170.0\n",
            "Step:8, Action:[3 0 0], State:[ 15.453125   10.        800.          2.5703125 200.          0.       ], Reward:-200.0\n",
            "Step:9, Action:[2 0 0], State:[ 14.7265625  10.        900.          2.5703125 220.          0.       ], Reward:-220.0\n",
            "Step:10, Action:[2 1 1], State:[ 14.36328125  10.         900.           2.5703125  240.\n",
            "   0.        ], Reward:-240.0\n",
            "Step:11, Action:[1 0 1], State:[ 13.18164062  10.         950.           2.5703125  250.\n",
            "   0.        ], Reward:-250.0\n",
            "Step:12, Action:[3 1 0], State:[  14.59082031   10.         1000.            2.5703125   280.\n",
            "    0.        ], Reward:-280.0\n",
            "Step:13, Action:[2 1 0], State:[  14.29541016   10.         1000.            2.5703125   300.\n",
            "    0.        ], Reward:-300.0\n",
            "Step:14, Action:[1 1 1], State:[ 13.14770508  10.         950.           2.5703125  310.\n",
            "   0.        ], Reward:-310.0\n",
            "Step:15, Action:[0 0 0], State:[ 11.57385254  10.         950.           2.5703125  310.\n",
            "   0.        ], Reward:-310.0\n",
            "Step:16, Action:[3 0 0], State:[  13.78692627   10.         1100.            2.5703125   340.\n",
            "    0.        ], Reward:-340.0\n",
            "Step:17, Action:[3 1 1], State:[  14.89346313   10.         1150.            2.5703125   370.\n",
            "    0.        ], Reward:-370.0\n",
            "Step:18, Action:[1 0 1], State:[  13.44673157   10.         1200.            2.5703125   380.\n",
            "    0.        ], Reward:-380.0\n",
            "Step:19, Action:[2 0 1], State:[  13.72336578   10.         1300.            2.5703125   400.\n",
            "    0.        ], Reward:-400.0\n",
            "Step:20, Action:[1 0 0], State:[  12.86168289   10.         1350.            2.5703125   410.\n",
            "    0.        ], Reward:-410.0\n",
            "Step:21, Action:[3 0 1], State:[  14.43084145   10.         1500.            2.5703125   440.\n",
            "    0.        ], Reward:-440.0\n",
            "Step:22, Action:[2 1 0], State:[  14.21542072   10.         1500.            2.5703125   460.\n",
            "    0.        ], Reward:-460.0\n",
            "Step:23, Action:[3 0 1], State:[  15.10771036   10.         1650.            2.62416768  490.\n",
            "    0.        ], Reward:-490.0\n",
            "Step:24, Action:[0 0 0], State:[  12.55385518   10.         1650.            2.62416768  490.\n",
            "    0.        ], Reward:-490.0\n",
            "Step:25, Action:[0 0 1], State:[  11.27692759   10.         1650.            2.62416768  490.\n",
            "    0.        ], Reward:-490.0\n",
            "Step:26, Action:[0 0 1], State:[  10.6384638    10.         1650.            2.62416768  490.\n",
            "    0.        ], Reward:-490.0\n",
            "Step:27, Action:[0 0 1], State:[  10.3192319    10.         1650.            2.62416768  490.\n",
            "    0.        ], Reward:-490.0\n",
            "Step:28, Action:[1 1 1], State:[  11.15961595   10.         1600.            2.62416768  500.\n",
            "    0.        ], Reward:-500.0\n",
            "Step:29, Action:[0 0 1], State:[  10.57980797   10.         1600.            2.62416768  500.\n",
            "    0.        ], Reward:-500.0\n",
            "Step:30, Action:[0 0 1], State:[  10.28990399   10.         1600.            2.62416768  500.\n",
            "    0.        ], Reward:-500.0\n",
            "Step:31, Action:[0 0 0], State:[  10.14495199   10.         1600.            2.62416768  500.\n",
            "    0.        ], Reward:-500.0\n",
            "Step:32, Action:[0 0 0], State:[  10.072476     10.         1600.            2.62416768  500.\n",
            "    0.        ], Reward:-500.0\n",
            "Step:33, Action:[1 0 1], State:[  11.036238     10.         1650.            2.62416768  510.\n",
            "    0.        ], Reward:-510.0\n",
            "Step:34, Action:[2 0 1], State:[  12.518119     10.         1750.            2.62416768  530.\n",
            "    0.        ], Reward:-530.0\n",
            "Step:35, Action:[2 0 0], State:[  13.2590595    10.         1850.            2.62416768  550.\n",
            "    0.        ], Reward:-550.0\n",
            "Step:36, Action:[1 0 1], State:[  12.62952975   10.         1900.            2.62416768  560.\n",
            "    0.        ], Reward:-560.0\n",
            "Step:37, Action:[2 0 0], State:[  13.31476487   10.         2000.            2.62416768  580.\n",
            "    0.        ], Reward:-580.0\n",
            "Step:38, Action:[2 0 1], State:[  13.65738244   10.         2100.            2.62416768  600.\n",
            "    0.        ], Reward:-600.0\n",
            "Step:39, Action:[0 0 1], State:[  11.82869122   10.         2100.            2.62416768  600.\n",
            "    0.        ], Reward:-600.0\n",
            "Step:40, Action:[1 1 0], State:[  11.91434561   10.         2050.            2.62416768  610.\n",
            "    0.        ], Reward:-610.0\n",
            "Step:41, Action:[1 1 1], State:[  11.9571728    10.         2000.            2.62416768  620.\n",
            "    0.        ], Reward:-620.0\n",
            "Step:42, Action:[2 1 0], State:[  12.9785864    10.         2000.            2.62416768  640.\n",
            "    0.        ], Reward:-640.0\n",
            "Step:43, Action:[0 0 1], State:[  11.4892932    10.         2000.            2.62416768  640.\n",
            "    0.        ], Reward:-640.0\n",
            "Step:44, Action:[1 1 0], State:[  11.7446466    10.         1950.            2.62416768  650.\n",
            "    0.        ], Reward:-650.0\n",
            "Step:45, Action:[1 1 0], State:[  11.8723233    10.         1900.            2.62416768  660.\n",
            "    0.        ], Reward:-660.0\n",
            "Step:46, Action:[0 0 1], State:[  10.93616165   10.         1900.            2.62416768  660.\n",
            "    0.        ], Reward:-660.0\n",
            "Step:47, Action:[3 0 1], State:[  13.46808083   10.         2050.            2.62416768  690.\n",
            "    0.        ], Reward:-690.0\n",
            "Step:48, Action:[2 0 1], State:[  13.73404041   10.         2150.            2.62416768  710.\n",
            "    0.        ], Reward:-710.0\n",
            "Step:49, Action:[2 1 0], State:[  13.86702021   10.         2150.            2.62416768  730.\n",
            "    0.        ], Reward:-730.0\n",
            "Step:50, Action:[3 0 1], State:[  14.9335101    10.         2300.            2.62416768  760.\n",
            "    0.        ], Reward:-760.0\n",
            "Step:51, Action:[2 1 0], State:[  14.46675505   10.         2300.            2.62416768  780.\n",
            "    0.        ], Reward:-780.0\n",
            "Step:52, Action:[1 0 0], State:[  13.23337753   10.         2350.            2.62416768  790.\n",
            "    0.        ], Reward:-790.0\n",
            "Step:53, Action:[2 0 0], State:[  13.61668876   10.         2450.            2.62416768  810.\n",
            "    0.        ], Reward:-810.0\n",
            "Step:54, Action:[0 0 1], State:[  11.80834438   10.         2450.            2.62416768  810.\n",
            "    0.        ], Reward:-810.0\n",
            "Step:55, Action:[0 1 1], State:[  10.90417219   10.         2350.            2.62416768  810.\n",
            "    0.        ], Reward:-810.0\n",
            "Step:56, Action:[2 1 1], State:[  12.4520861    10.         2350.            2.62416768  830.\n",
            "    0.        ], Reward:-830.0\n",
            "Step:57, Action:[1 1 0], State:[  12.22604305   10.         2300.            2.62416768  840.\n",
            "    0.        ], Reward:-840.0\n",
            "Step:58, Action:[3 1 1], State:[  14.11302152   10.         2350.            2.62416768  870.\n",
            "    0.        ], Reward:-870.0\n",
            "Step:59, Action:[2 0 1], State:[  14.05651076   10.         2450.            2.62416768  890.\n",
            "    0.        ], Reward:-890.0\n",
            "Step:60, Action:[3 0 0], State:[  15.02825538   10.         2600.            2.63829537  920.\n",
            "    0.        ], Reward:-920.0\n",
            "Step:61, Action:[3 1 1], State:[  15.51412769   10.         2650.            2.89535922  950.\n",
            "    0.        ], Reward:-950.0\n",
            "Step:62, Action:[0 1 0], State:[  12.75706385   10.         2550.            2.89535922  950.\n",
            "    0.        ], Reward:-950.0\n",
            "Step:63, Action:[2 1 1], State:[  13.37853192   10.         2550.            2.89535922  970.\n",
            "    0.        ], Reward:-970.0\n",
            "Step:64, Action:[2 0 1], State:[  13.68926596   10.         2650.            2.89535922  990.\n",
            "    0.        ], Reward:-990.0\n",
            "Step:65, Action:[2 0 0], State:[  13.84463298   10.         2750.            2.89535922 1010.\n",
            "    0.        ], Reward:-1010.0\n",
            "Step:66, Action:[2 1 1], State:[  13.92231649   10.         2750.            2.89535922 1030.\n",
            "    0.        ], Reward:-1030.0\n",
            "Step:67, Action:[1 1 1], State:[  12.96115825   10.         2700.            2.89535922 1040.\n",
            "    0.        ], Reward:-1040.0\n",
            "Step:68, Action:[3 1 1], State:[  14.48057912   10.         2750.            2.89535922 1070.\n",
            "    0.        ], Reward:-1070.0\n",
            "Step:69, Action:[2 0 1], State:[  14.24028956   10.         2850.            2.89535922 1090.\n",
            "    0.        ], Reward:-1090.0\n",
            "Step:70, Action:[1 1 0], State:[  13.12014478   10.         2800.            2.89535922 1100.\n",
            "    0.        ], Reward:-1100.0\n",
            "Step:71, Action:[2 1 0], State:[  13.56007239   10.         2800.            2.89535922 1120.\n",
            "    0.        ], Reward:-1120.0\n",
            "Step:72, Action:[1 0 1], State:[  12.7800362    10.         2850.            2.89535922 1130.\n",
            "    0.        ], Reward:-1130.0\n",
            "Step:73, Action:[3 0 1], State:[1.43900181e+01 1.00000000e+01 3.00000000e+03 2.89535922e+00\n",
            " 1.16000000e+03 0.00000000e+00], Reward:-1160.0\n",
            "Step:74, Action:[3 1 0], State:[1.51950090e+01 1.00000000e+01 3.05000000e+03 2.99286374e+00\n",
            " 1.19000000e+03 0.00000000e+00], Reward:-1190.0\n",
            "Step:75, Action:[0 1 0], State:[  12.59750452   10.         2950.            2.99286374 1190.\n",
            "    0.        ], Reward:-1190.0\n",
            "Step:76, Action:[3 0 1], State:[1.42987523e+01 1.00000000e+01 3.10000000e+03 2.99286374e+00\n",
            " 1.22000000e+03 0.00000000e+00], Reward:-1220.0\n",
            "Step:77, Action:[0 1 1], State:[1.21493761e+01 1.00000000e+01 3.00000000e+03 2.99286374e+00\n",
            " 1.22000000e+03 0.00000000e+00], Reward:-1220.0\n",
            "Step:78, Action:[0 0 1], State:[1.10746881e+01 1.00000000e+01 3.00000000e+03 2.99286374e+00\n",
            " 1.22000000e+03 0.00000000e+00], Reward:-1220.0\n",
            "Step:79, Action:[2 1 1], State:[1.25373440e+01 1.00000000e+01 3.00000000e+03 2.99286374e+00\n",
            " 1.24000000e+03 0.00000000e+00], Reward:-1240.0\n",
            "Step:80, Action:[3 0 1], State:[1.42686720e+01 1.00000000e+01 3.15000000e+03 2.99286374e+00\n",
            " 1.27000000e+03 0.00000000e+00], Reward:-1270.0\n",
            "Step:81, Action:[3 1 1], State:[1.51343360e+01 1.00000000e+01 3.20000000e+03 3.06003174e+00\n",
            " 1.30000000e+03 0.00000000e+00], Reward:-1300.0\n",
            "Step:82, Action:[2 0 1], State:[1.45671680e+01 1.00000000e+01 3.30000000e+03 3.06003174e+00\n",
            " 1.32000000e+03 0.00000000e+00], Reward:-1320.0\n",
            "Step:83, Action:[1 1 0], State:[1.32835840e+01 1.00000000e+01 3.25000000e+03 3.06003174e+00\n",
            " 1.33000000e+03 0.00000000e+00], Reward:-1330.0\n",
            "Step:84, Action:[3 0 1], State:[1.46417920e+01 1.00000000e+01 3.40000000e+03 3.06003174e+00\n",
            " 1.36000000e+03 0.00000000e+00], Reward:-1360.0\n",
            "Step:85, Action:[0 0 1], State:[1.23208960e+01 1.00000000e+01 3.40000000e+03 3.06003174e+00\n",
            " 1.36000000e+03 0.00000000e+00], Reward:-1360.0\n",
            "Step:86, Action:[1 1 1], State:[1.21604480e+01 1.00000000e+01 3.35000000e+03 3.06003174e+00\n",
            " 1.37000000e+03 0.00000000e+00], Reward:-1370.0\n",
            "Step:87, Action:[0 1 1], State:[1.10802240e+01 1.00000000e+01 3.25000000e+03 3.06003174e+00\n",
            " 1.37000000e+03 0.00000000e+00], Reward:-1370.0\n",
            "Step:88, Action:[0 1 0], State:[1.05401120e+01 1.00000000e+01 3.15000000e+03 3.06003174e+00\n",
            " 1.37000000e+03 0.00000000e+00], Reward:-1370.0\n",
            "Step:89, Action:[1 1 0], State:[1.12700560e+01 1.00000000e+01 3.10000000e+03 3.06003174e+00\n",
            " 1.38000000e+03 0.00000000e+00], Reward:-1380.0\n",
            "Step:90, Action:[1 0 0], State:[1.16350280e+01 1.00000000e+01 3.15000000e+03 3.06003174e+00\n",
            " 1.39000000e+03 0.00000000e+00], Reward:-1390.0\n",
            "Step:91, Action:[3 0 1], State:[1.38175140e+01 1.00000000e+01 3.30000000e+03 3.06003174e+00\n",
            " 1.42000000e+03 0.00000000e+00], Reward:-1420.0\n",
            "Step:92, Action:[1 1 1], State:[1.29087570e+01 1.00000000e+01 3.25000000e+03 3.06003174e+00\n",
            " 1.43000000e+03 0.00000000e+00], Reward:-1430.0\n",
            "Step:93, Action:[2 0 0], State:[1.34543785e+01 1.00000000e+01 3.35000000e+03 3.06003174e+00\n",
            " 1.45000000e+03 0.00000000e+00], Reward:-1450.0\n",
            "Step:94, Action:[3 0 1], State:[1.47271893e+01 1.00000000e+01 3.50000000e+03 3.06003174e+00\n",
            " 1.48000000e+03 0.00000000e+00], Reward:-1480.0\n",
            "Step:95, Action:[2 1 1], State:[1.43635946e+01 1.00000000e+01 3.50000000e+03 3.06003174e+00\n",
            " 1.50000000e+03 0.00000000e+00], Reward:-1500.0\n",
            "Step:96, Action:[3 1 0], State:[1.51817973e+01 1.00000000e+01 3.55000000e+03 3.15093040e+00\n",
            " 1.53000000e+03 0.00000000e+00], Reward:-1530.0\n",
            "Step:97, Action:[3 0 0], State:[1.55908987e+01 1.00000000e+01 3.70000000e+03 3.44637973e+00\n",
            " 1.56000000e+03 0.00000000e+00], Reward:-1560.0\n",
            "Step:98, Action:[0 1 1], State:[1.27954493e+01 1.00000000e+01 3.60000000e+03 3.44637973e+00\n",
            " 1.56000000e+03 0.00000000e+00], Reward:-1560.0\n",
            "Step:99, Action:[1 1 0], State:[1.23977247e+01 1.00000000e+01 3.55000000e+03 3.44637973e+00\n",
            " 1.57000000e+03 0.00000000e+00], Reward:-1570.0\n",
            "Step:100, Action:[3 0 1], State:[1.41988623e+01 1.00000000e+01 3.70000000e+03 3.44637973e+00\n",
            " 1.60000000e+03 0.00000000e+00], Reward:-1600.0\n",
            "Step:101, Action:[0 1 1], State:[1.20994312e+01 1.00000000e+01 3.60000000e+03 3.44637973e+00\n",
            " 1.60000000e+03 0.00000000e+00], Reward:-1600.0\n",
            "Step:102, Action:[1 0 1], State:[1.20497156e+01 1.00000000e+01 3.65000000e+03 3.44637973e+00\n",
            " 1.61000000e+03 0.00000000e+00], Reward:-1610.0\n",
            "Step:103, Action:[0 0 1], State:[1.10248578e+01 1.00000000e+01 3.65000000e+03 3.44637973e+00\n",
            " 1.61000000e+03 0.00000000e+00], Reward:-1610.0\n",
            "Step:104, Action:[1 0 0], State:[1.15124289e+01 1.00000000e+01 3.70000000e+03 3.44637973e+00\n",
            " 1.62000000e+03 0.00000000e+00], Reward:-1620.0\n",
            "Step:105, Action:[3 0 0], State:[1.37562144e+01 1.00000000e+01 3.85000000e+03 3.44637973e+00\n",
            " 1.65000000e+03 0.00000000e+00], Reward:-1650.0\n",
            "Step:106, Action:[3 1 1], State:[1.48781072e+01 1.00000000e+01 3.90000000e+03 3.44637973e+00\n",
            " 1.68000000e+03 0.00000000e+00], Reward:-1680.0\n",
            "Step:107, Action:[3 1 1], State:[1.54390536e+01 1.00000000e+01 3.95000000e+03 3.66590654e+00\n",
            " 1.71000000e+03 0.00000000e+00], Reward:-1710.0\n",
            "Step:108, Action:[1 1 1], State:[1.37195268e+01 1.00000000e+01 3.90000000e+03 3.66590654e+00\n",
            " 1.72000000e+03 0.00000000e+00], Reward:-1720.0\n",
            "Step:109, Action:[1 0 0], State:[1.28597634e+01 1.00000000e+01 3.95000000e+03 3.66590654e+00\n",
            " 1.73000000e+03 0.00000000e+00], Reward:-1730.0\n",
            "Step:110, Action:[1 0 0], State:[1.24298817e+01 1.00000000e+01 4.00000000e+03 3.66590654e+00\n",
            " 1.74000000e+03 0.00000000e+00], Reward:-1740.0\n",
            "Step:111, Action:[0 1 0], State:[1.12149409e+01 1.00000000e+01 3.90000000e+03 3.66590654e+00\n",
            " 1.74000000e+03 0.00000000e+00], Reward:-1740.0\n",
            "Step:112, Action:[1 1 0], State:[1.16074704e+01 1.00000000e+01 3.85000000e+03 3.66590654e+00\n",
            " 1.75000000e+03 0.00000000e+00], Reward:-1750.0\n",
            "Step:113, Action:[1 0 1], State:[1.18037352e+01 1.00000000e+01 3.90000000e+03 3.66590654e+00\n",
            " 1.76000000e+03 0.00000000e+00], Reward:-1760.0\n",
            "Step:114, Action:[1 0 1], State:[1.19018676e+01 1.00000000e+01 3.95000000e+03 3.66590654e+00\n",
            " 1.77000000e+03 0.00000000e+00], Reward:-1770.0\n",
            "Step:115, Action:[2 0 0], State:[1.29509338e+01 1.00000000e+01 4.05000000e+03 3.66590654e+00\n",
            " 1.79000000e+03 0.00000000e+00], Reward:-1790.0\n",
            "Step:116, Action:[2 0 1], State:[1.34754669e+01 1.00000000e+01 4.15000000e+03 3.66590654e+00\n",
            " 1.81000000e+03 0.00000000e+00], Reward:-1810.0\n",
            "Step:117, Action:[3 1 0], State:[1.47377335e+01 1.00000000e+01 4.20000000e+03 3.66590654e+00\n",
            " 1.84000000e+03 0.00000000e+00], Reward:-1840.0\n",
            "Step:118, Action:[3 0 0], State:[1.53688667e+01 1.00000000e+01 4.35000000e+03 3.85033990e+00\n",
            " 1.87000000e+03 0.00000000e+00], Reward:-1870.0\n",
            "Step:119, Action:[0 0 0], State:[1.26844334e+01 1.00000000e+01 4.35000000e+03 3.85033990e+00\n",
            " 1.87000000e+03 0.00000000e+00], Reward:-1870.0\n",
            "Step:120, Action:[1 1 1], State:[1.23422167e+01 1.00000000e+01 4.30000000e+03 3.85033990e+00\n",
            " 1.88000000e+03 0.00000000e+00], Reward:-1880.0\n",
            "Step:121, Action:[1 1 1], State:[1.21711083e+01 1.00000000e+01 4.25000000e+03 3.85033990e+00\n",
            " 1.89000000e+03 0.00000000e+00], Reward:-1890.0\n",
            "Step:122, Action:[0 1 1], State:[1.10855542e+01 1.00000000e+01 4.15000000e+03 3.85033990e+00\n",
            " 1.89000000e+03 0.00000000e+00], Reward:-1890.0\n",
            "Step:123, Action:[1 0 1], State:[1.15427771e+01 1.00000000e+01 4.20000000e+03 3.85033990e+00\n",
            " 1.90000000e+03 0.00000000e+00], Reward:-1900.0\n",
            "Step:124, Action:[2 1 1], State:[1.27713885e+01 1.00000000e+01 4.20000000e+03 3.85033990e+00\n",
            " 1.92000000e+03 0.00000000e+00], Reward:-1920.0\n",
            "Step:125, Action:[0 0 1], State:[1.13856943e+01 1.00000000e+01 4.20000000e+03 3.85033990e+00\n",
            " 1.92000000e+03 0.00000000e+00], Reward:-1920.0\n",
            "Step:126, Action:[3 0 1], State:[1.36928471e+01 1.00000000e+01 4.35000000e+03 3.85033990e+00\n",
            " 1.95000000e+03 0.00000000e+00], Reward:-1950.0\n",
            "Step:127, Action:[3 0 0], State:[1.48464236e+01 1.00000000e+01 4.50000000e+03 3.85033990e+00\n",
            " 1.98000000e+03 0.00000000e+00], Reward:-1980.0\n",
            "Step:128, Action:[0 0 0], State:[1.24232118e+01 1.00000000e+01 4.50000000e+03 3.85033990e+00\n",
            " 1.98000000e+03 0.00000000e+00], Reward:-1980.0\n",
            "Step:129, Action:[3 0 0], State:[1.42116059e+01 1.00000000e+01 4.65000000e+03 3.85033990e+00\n",
            " 2.01000000e+03 0.00000000e+00], Reward:-2010.0\n",
            "Step:130, Action:[3 1 1], State:[1.51058029e+01 1.00000000e+01 4.70000000e+03 3.90324137e+00\n",
            " 2.04000000e+03 0.00000000e+00], Reward:-2040.0\n",
            "Step:131, Action:[2 1 0], State:[1.45529015e+01 1.00000000e+01 4.70000000e+03 3.90324137e+00\n",
            " 2.06000000e+03 0.00000000e+00], Reward:-2060.0\n",
            "Step:132, Action:[1 0 0], State:[1.32764507e+01 1.00000000e+01 4.75000000e+03 3.90324137e+00\n",
            " 2.07000000e+03 0.00000000e+00], Reward:-2070.0\n",
            "Step:133, Action:[2 0 0], State:[1.36382254e+01 1.00000000e+01 4.85000000e+03 3.90324137e+00\n",
            " 2.09000000e+03 0.00000000e+00], Reward:-2090.0\n",
            "Step:134, Action:[2 0 0], State:[1.38191127e+01 1.00000000e+01 4.95000000e+03 3.90324137e+00\n",
            " 2.11000000e+03 0.00000000e+00], Reward:-2110.0\n",
            "Step:135, Action:[3 0 1], State:[1.49095563e+01 1.00000000e+01 5.10000000e+03 3.90324137e+00\n",
            " 2.14000000e+03 0.00000000e+00], Reward:-2140.0\n",
            "Step:136, Action:[0 0 1], State:[1.24547782e+01 1.00000000e+01 5.10000000e+03 3.90324137e+00\n",
            " 2.14000000e+03 0.00000000e+00], Reward:-2140.0\n",
            "Step:137, Action:[0 0 1], State:[1.12273891e+01 1.00000000e+01 5.10000000e+03 3.90324137e+00\n",
            " 2.14000000e+03 0.00000000e+00], Reward:-2140.0\n",
            "Step:138, Action:[1 0 1], State:[1.16136945e+01 1.00000000e+01 5.15000000e+03 3.90324137e+00\n",
            " 2.15000000e+03 0.00000000e+00], Reward:-2150.0\n",
            "Step:139, Action:[0 0 1], State:[1.08068473e+01 1.00000000e+01 5.15000000e+03 3.90324137e+00\n",
            " 2.15000000e+03 0.00000000e+00], Reward:-2150.0\n",
            "Step:140, Action:[3 0 0], State:[1.34034236e+01 1.00000000e+01 5.30000000e+03 3.90324137e+00\n",
            " 2.18000000e+03 0.00000000e+00], Reward:-2180.0\n",
            "Step:141, Action:[3 1 0], State:[1.47017118e+01 1.00000000e+01 5.35000000e+03 3.90324137e+00\n",
            " 2.21000000e+03 0.00000000e+00], Reward:-2210.0\n",
            "Step:142, Action:[2 1 0], State:[1.43508559e+01 1.00000000e+01 5.35000000e+03 3.90324137e+00\n",
            " 2.23000000e+03 0.00000000e+00], Reward:-2230.0\n",
            "Step:143, Action:[1 1 0], State:[1.31754280e+01 1.00000000e+01 5.30000000e+03 3.90324137e+00\n",
            " 2.24000000e+03 0.00000000e+00], Reward:-2240.0\n",
            "Step:144, Action:[0 1 0], State:[1.15877140e+01 1.00000000e+01 5.20000000e+03 3.90324137e+00\n",
            " 2.24000000e+03 0.00000000e+00], Reward:-2240.0\n",
            "Step:145, Action:[0 0 1], State:[1.07938570e+01 1.00000000e+01 5.20000000e+03 3.90324137e+00\n",
            " 2.24000000e+03 0.00000000e+00], Reward:-2240.0\n",
            "Step:146, Action:[2 1 1], State:[1.23969285e+01 1.00000000e+01 5.20000000e+03 3.90324137e+00\n",
            " 2.26000000e+03 0.00000000e+00], Reward:-2260.0\n",
            "Step:147, Action:[0 0 0], State:[1.11984642e+01 1.00000000e+01 5.20000000e+03 3.90324137e+00\n",
            " 2.26000000e+03 0.00000000e+00], Reward:-2260.0\n",
            "Step:148, Action:[3 1 0], State:[1.35992321e+01 1.00000000e+01 5.25000000e+03 3.90324137e+00\n",
            " 2.29000000e+03 0.00000000e+00], Reward:-2290.0\n",
            "Step:149, Action:[3 0 0], State:[1.47996161e+01 1.00000000e+01 5.40000000e+03 3.90324137e+00\n",
            " 2.32000000e+03 0.00000000e+00], Reward:-2320.0\n",
            "Step:150, Action:[1 0 1], State:[1.33998080e+01 1.00000000e+01 5.45000000e+03 3.90324137e+00\n",
            " 2.33000000e+03 0.00000000e+00], Reward:-2330.0\n",
            "Step:151, Action:[0 1 1], State:[1.16999040e+01 1.00000000e+01 5.35000000e+03 3.90324137e+00\n",
            " 2.33000000e+03 0.00000000e+00], Reward:-2330.0\n",
            "Step:152, Action:[0 1 0], State:[1.08499520e+01 1.00000000e+01 5.25000000e+03 3.90324137e+00\n",
            " 2.33000000e+03 0.00000000e+00], Reward:-2330.0\n",
            "Step:153, Action:[0 1 1], State:[1.04249760e+01 1.00000000e+01 5.15000000e+03 3.90324137e+00\n",
            " 2.33000000e+03 0.00000000e+00], Reward:-2330.0\n",
            "Step:154, Action:[0 1 1], State:[1.02124880e+01 1.00000000e+01 5.05000000e+03 3.90324137e+00\n",
            " 2.33000000e+03 0.00000000e+00], Reward:-2330.0\n",
            "Step:155, Action:[1 0 1], State:[1.11062440e+01 1.00000000e+01 5.10000000e+03 3.90324137e+00\n",
            " 2.34000000e+03 0.00000000e+00], Reward:-2340.0\n",
            "Step:156, Action:[0 0 1], State:[1.05531220e+01 1.00000000e+01 5.10000000e+03 3.90324137e+00\n",
            " 2.34000000e+03 0.00000000e+00], Reward:-2340.0\n",
            "Step:157, Action:[0 1 1], State:[1.02765610e+01 1.00000000e+01 5.00000000e+03 3.90324137e+00\n",
            " 2.34000000e+03 0.00000000e+00], Reward:-2340.0\n",
            "Step:158, Action:[1 1 0], State:[1.11382805e+01 1.00000000e+01 4.95000000e+03 3.90324137e+00\n",
            " 2.35000000e+03 0.00000000e+00], Reward:-2350.0\n",
            "Step:159, Action:[3 1 1], State:[1.35691403e+01 1.00000000e+01 5.00000000e+03 3.90324137e+00\n",
            " 2.38000000e+03 0.00000000e+00], Reward:-2380.0\n",
            "Step:160, Action:[1 0 1], State:[1.27845701e+01 1.00000000e+01 5.05000000e+03 3.90324137e+00\n",
            " 2.39000000e+03 0.00000000e+00], Reward:-2390.0\n",
            "Step:161, Action:[1 0 1], State:[1.23922851e+01 1.00000000e+01 5.10000000e+03 3.90324137e+00\n",
            " 2.40000000e+03 0.00000000e+00], Reward:-2400.0\n",
            "Step:162, Action:[0 0 0], State:[1.11961425e+01 1.00000000e+01 5.10000000e+03 3.90324137e+00\n",
            " 2.40000000e+03 0.00000000e+00], Reward:-2400.0\n",
            "Step:163, Action:[3 0 0], State:[1.35980713e+01 1.00000000e+01 5.25000000e+03 3.90324137e+00\n",
            " 2.43000000e+03 0.00000000e+00], Reward:-2430.0\n",
            "Step:164, Action:[1 1 0], State:[1.27990356e+01 1.00000000e+01 5.20000000e+03 3.90324137e+00\n",
            " 2.44000000e+03 0.00000000e+00], Reward:-2440.0\n",
            "Step:165, Action:[1 0 1], State:[1.23995178e+01 1.00000000e+01 5.25000000e+03 3.90324137e+00\n",
            " 2.45000000e+03 0.00000000e+00], Reward:-2450.0\n",
            "Step:166, Action:[3 0 0], State:[1.41997589e+01 1.00000000e+01 5.40000000e+03 3.90324137e+00\n",
            " 2.48000000e+03 0.00000000e+00], Reward:-2480.0\n",
            "Step:167, Action:[2 1 1], State:[1.40998795e+01 1.00000000e+01 5.40000000e+03 3.90324137e+00\n",
            " 2.50000000e+03 0.00000000e+00], Reward:-2500.0\n",
            "Step:168, Action:[3 0 0], State:[1.50499397e+01 1.00000000e+01 5.55000000e+03 3.92821123e+00\n",
            " 2.53000000e+03 0.00000000e+00], Reward:-2530.0\n",
            "Step:169, Action:[0 0 0], State:[1.25249699e+01 1.00000000e+01 5.55000000e+03 3.92821123e+00\n",
            " 2.53000000e+03 0.00000000e+00], Reward:-2530.0\n",
            "Step:170, Action:[2 1 1], State:[1.32624849e+01 1.00000000e+01 5.55000000e+03 3.92821123e+00\n",
            " 2.55000000e+03 0.00000000e+00], Reward:-2550.0\n",
            "Step:171, Action:[2 1 0], State:[1.36312425e+01 1.00000000e+01 5.55000000e+03 3.92821123e+00\n",
            " 2.57000000e+03 0.00000000e+00], Reward:-2570.0\n",
            "Step:172, Action:[1 0 1], State:[1.28156212e+01 1.00000000e+01 5.60000000e+03 3.92821123e+00\n",
            " 2.58000000e+03 0.00000000e+00], Reward:-2580.0\n",
            "Step:173, Action:[0 1 0], State:[1.14078106e+01 1.00000000e+01 5.50000000e+03 3.92821123e+00\n",
            " 2.58000000e+03 0.00000000e+00], Reward:-2580.0\n",
            "Step:174, Action:[1 1 0], State:[1.17039053e+01 1.00000000e+01 5.45000000e+03 3.92821123e+00\n",
            " 2.59000000e+03 0.00000000e+00], Reward:-2590.0\n",
            "Step:175, Action:[2 1 1], State:[1.28519527e+01 1.00000000e+01 5.45000000e+03 3.92821123e+00\n",
            " 2.61000000e+03 0.00000000e+00], Reward:-2610.0\n",
            "Step:176, Action:[3 1 0], State:[1.44259763e+01 1.00000000e+01 5.50000000e+03 3.92821123e+00\n",
            " 2.64000000e+03 0.00000000e+00], Reward:-2640.0\n",
            "Step:177, Action:[3 1 1], State:[1.52129882e+01 1.00000000e+01 5.55000000e+03 4.03470532e+00\n",
            " 2.67000000e+03 0.00000000e+00], Reward:-2670.0\n",
            "Step:178, Action:[1 1 0], State:[1.36064941e+01 1.00000000e+01 5.50000000e+03 4.03470532e+00\n",
            " 2.68000000e+03 0.00000000e+00], Reward:-2680.0\n",
            "Step:179, Action:[2 1 0], State:[1.38032470e+01 1.00000000e+01 5.50000000e+03 4.03470532e+00\n",
            " 2.70000000e+03 0.00000000e+00], Reward:-2700.0\n",
            "Step:180, Action:[0 1 1], State:[1.19016235e+01 1.00000000e+01 5.40000000e+03 4.03470532e+00\n",
            " 2.70000000e+03 0.00000000e+00], Reward:-2700.0\n",
            "Step:181, Action:[0 1 1], State:[1.09508118e+01 1.00000000e+01 5.30000000e+03 4.03470532e+00\n",
            " 2.70000000e+03 0.00000000e+00], Reward:-2700.0\n",
            "Step:182, Action:[0 0 1], State:[1.04754059e+01 1.00000000e+01 5.30000000e+03 4.03470532e+00\n",
            " 2.70000000e+03 0.00000000e+00], Reward:-2700.0\n",
            "Step:183, Action:[1 0 1], State:[1.12377029e+01 1.00000000e+01 5.35000000e+03 4.03470532e+00\n",
            " 2.71000000e+03 0.00000000e+00], Reward:-2710.0\n",
            "Step:184, Action:[1 0 0], State:[1.16188515e+01 1.00000000e+01 5.40000000e+03 4.03470532e+00\n",
            " 2.72000000e+03 0.00000000e+00], Reward:-2720.0\n",
            "Step:185, Action:[0 0 0], State:[1.08094257e+01 1.00000000e+01 5.40000000e+03 4.03470532e+00\n",
            " 2.72000000e+03 0.00000000e+00], Reward:-2720.0\n",
            "Step:186, Action:[1 1 1], State:[1.14047129e+01 1.00000000e+01 5.35000000e+03 4.03470532e+00\n",
            " 2.73000000e+03 0.00000000e+00], Reward:-2730.0\n",
            "Step:187, Action:[2 0 1], State:[1.27023564e+01 1.00000000e+01 5.45000000e+03 4.03470532e+00\n",
            " 2.75000000e+03 0.00000000e+00], Reward:-2750.0\n",
            "Step:188, Action:[3 1 1], State:[1.43511782e+01 1.00000000e+01 5.50000000e+03 4.03470532e+00\n",
            " 2.78000000e+03 0.00000000e+00], Reward:-2780.0\n",
            "Step:189, Action:[2 1 1], State:[1.41755891e+01 1.00000000e+01 5.50000000e+03 4.03470532e+00\n",
            " 2.80000000e+03 0.00000000e+00], Reward:-2800.0\n",
            "Step:190, Action:[2 0 0], State:[1.40877946e+01 1.00000000e+01 5.60000000e+03 4.03470532e+00\n",
            " 2.82000000e+03 0.00000000e+00], Reward:-2820.0\n",
            "Step:191, Action:[2 1 0], State:[1.40438973e+01 1.00000000e+01 5.60000000e+03 4.03470532e+00\n",
            " 2.84000000e+03 0.00000000e+00], Reward:-2840.0\n",
            "Step:192, Action:[3 1 1], State:[1.50219486e+01 1.00000000e+01 5.65000000e+03 4.04567964e+00\n",
            " 2.87000000e+03 0.00000000e+00], Reward:-2870.0\n",
            "Step:193, Action:[1 1 1], State:[1.35109743e+01 1.00000000e+01 5.60000000e+03 4.04567964e+00\n",
            " 2.88000000e+03 0.00000000e+00], Reward:-2880.0\n",
            "Step:194, Action:[3 1 0], State:[1.47554872e+01 1.00000000e+01 5.65000000e+03 4.04567964e+00\n",
            " 2.91000000e+03 0.00000000e+00], Reward:-2910.0\n",
            "Step:195, Action:[2 0 1], State:[1.43777436e+01 1.00000000e+01 5.75000000e+03 4.04567964e+00\n",
            " 2.93000000e+03 0.00000000e+00], Reward:-2930.0\n",
            "Step:196, Action:[1 1 0], State:[1.31888718e+01 1.00000000e+01 5.70000000e+03 4.04567964e+00\n",
            " 2.94000000e+03 0.00000000e+00], Reward:-2940.0\n",
            "Step:197, Action:[3 0 0], State:[1.45944359e+01 1.00000000e+01 5.85000000e+03 4.04567964e+00\n",
            " 2.97000000e+03 0.00000000e+00], Reward:-2970.0\n",
            "Step:198, Action:[2 0 1], State:[1.42972179e+01 1.00000000e+01 5.95000000e+03 4.04567964e+00\n",
            " 2.99000000e+03 0.00000000e+00], Reward:-2990.0\n",
            "Step:199, Action:[3 1 0], State:[1.51486090e+01 1.00000000e+01 6.00000000e+03 4.11998412e+00\n",
            " 3.02000000e+03 0.00000000e+00], Reward:-3020.0\n",
            "Step:200, Action:[3 1 1], State:[1.55743045e+01 1.00000000e+01 6.05000000e+03 4.40713637e+00\n",
            " 3.05000000e+03 0.00000000e+00], Reward:-3050.0\n",
            "Total Reward: -310380.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "class TomatoGreenhouseEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # state variables [Internal Temperature, external, CO2, maturity, production cost, inventory]\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0, -10, 400, 0, 0, 0]),\n",
        "            high=np.array([40, 40, 2000, 100, np.inf, np.inf]),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "        # defining agent actions [(0=weak,1=,2=medium,3=strong), Ventilation(0/1), harvest(0/1)]\n",
        "        self.action_space = spaces.MultiDiscrete([4,2,2])\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.array([20, 10, 400, 0, 0, 0])  #initialization\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        temp, ext_temp, co2, maturity, cost, stock = self.state\n",
        "        heating_level, ventilation, harvest = action\n",
        "\n",
        "        # gas heating\n",
        "        gas_consumption = [0, 1, 2, 3][heating_level]\n",
        "        cost += gas_consumption * 10  # 비용 계산\n",
        "\n",
        "        # CO2 control\n",
        "        temp += 0.5*(ext_temp - temp) + gas_consumption*1.0\n",
        "        co2 += gas_consumption*50 - ventilation*100\n",
        "\n",
        "        # maturity\n",
        "        maturity += max(0, (temp - 15)*0.5)\n",
        "\n",
        "        reward = 0\n",
        "        # harvest decision\n",
        "        if harvest and maturity >= 90:\n",
        "            harvested_amount = maturity * 0.5  # productivity calculation\n",
        "            stock += harvested_amount\n",
        "            reward += harvested_amount * 200  # revenue of sales\n",
        "            maturity = 0  #initialization\n",
        "\n",
        "        # reward から　費用マイナス\n",
        "        reward -= cost\n",
        "\n",
        "        # state update\n",
        "\n",
        "        self.state = np.array([temp, ext_temp, co2, maturity, cost, stock])\n",
        "\n",
        "        done = False\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "!pip install stable-baselines3\n",
        "!pip install gym\n",
        "!pip install 'shimmy>=2.0'\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "# 이미 정의한 환경 클래스 불러오기\n",
        "class TomatoGreenhouseEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=np.array([0, -10, 400, 0, 0, 0]),\n",
        "            high=np.array([40, 40, 2000, 100, np.inf, np.inf]),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "        self.action_space = gym.spaces.MultiDiscrete([4,2,2])\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.array([20, 10, 400, 0, 0, 0])\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        temp, ext_temp, co2, maturity, cost, stock = self.state\n",
        "        heating_level, ventilation, harvest = action\n",
        "        gas_consumption = [0, 1, 2, 3][heating_level]\n",
        "        cost += gas_consumption * 10\n",
        "        temp += 0.5*(ext_temp - temp) + gas_consumption*1.0\n",
        "        co2 += gas_consumption*50 - ventilation*100\n",
        "        maturity += max(0, (temp - 15)*0.5)\n",
        "        reward = 0\n",
        "        if harvest and maturity >= 90:\n",
        "            harvested_amount = maturity * 0.5\n",
        "            stock += harvested_amount\n",
        "            reward += harvested_amount * 200\n",
        "            maturity = 0\n",
        "        reward -= cost\n",
        "        self.state = np.array([temp, ext_temp, co2, maturity, cost, stock])\n",
        "        done = False\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "# 환경 생성\n",
        "env = TomatoGreenhouseEnv()\n",
        "\n",
        "# PPO 에이전트 모델 생성\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "\n",
        "# 학습 수행 (100,000회 반복)\n",
        "model.learn(total_timesteps=100000)\n",
        "\n",
        "# 학습 모델 저장 (나중에 사용가능)\n",
        "model.save(\"tomato_greenhouse_agent\")\n",
        "\n",
        "# 저장된 모델 로드\n",
        "model = PPO.load(\"tomato_greenhouse_agent\")\n",
        "\n",
        "# 환경 초기화\n",
        "obs = env.reset()\n",
        "total_reward = 0\n",
        "\n",
        "# 시뮬레이션 수행 (200단계 동안)\n",
        "for step in range(200):\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "    print(f\"Step:{step+1}, Action:{action}, State:{obs}, Reward:{reward}\")\n",
        "\n",
        "print(f\"Total Reward: {total_reward}\")"
      ]
    }
  ]
}